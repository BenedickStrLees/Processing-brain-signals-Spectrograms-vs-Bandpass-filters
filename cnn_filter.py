# -*- coding: utf-8 -*-
"""cnn_filter.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LEReEdpiSxMnWX-5XO_icfraY-01Kt36
"""

import librosa
import librosa.display as lida
import numpy as np
import pandas as pd
import os
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.optimizers import Adadelta
from keras.utils.np_utils import to_categorical
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, AvgPool2D, BatchNormalization, Reshape
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import LearningRateScheduler
import scipy.io as spio
import torch.nn as nn
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score

data_path='/content/drive/MyDrive/Coursework_C/'

mat = spio.loadmat(data_path + 'training.mat', squeeze_me=True)
d = mat['d']
Index = mat['Index']
Class = mat['Class']
Index_sorted=sorted(Index)

plt.title('Signal in real time')
plt.xlabel('time  (seconds)')
plt.ylabel('amplitude  (mV)')
plt.plot(np.array(range(len(d[:1000])))/25000, d[:1000])
plt.show()

def slice_into_frames(amplitudes, window_length, hop_length):
    return librosa.core.spectrum.util.frame(
        np.pad(amplitudes, int(window_length // 2), mode='reflect'),
        frame_length=window_length, hop_length=hop_length)
    # returns [window_length, num_windows]

"""The fast Fourier transform (FFT) is an algorithm that can efficiently compute the Fourier transform. """

def get_STFT(amplitudes, window_length, hop_length):
    # slicing into intersecting frames [window_length, num_frames]
    frames = slice_into_frames(amplitudes, window_length, hop_length)
    
    # getting weights for Fourier, float[window_length]
    fft_weights = librosa.core.spectrum.get_window('hann', window_length, fftbins=True)
    
    # transforming with Fourier
    stft = np.fft.rfft(frames * fft_weights[:, None], axis=0)
    return stft

window=13
hop=1
stft = get_STFT(d,window,hop)
spectrogram = np.abs(stft ** 2)
spectrogram = librosa.amplitude_to_db(spectrogram, ref=np.max)
lida.specshow(spectrogram, sr=25000,hop_length=hop, x_axis='s', y_axis='hz');
plt.title('Spectrogram')
plt.show()

plt.title('Signal in real time')
plt.xlabel('time  (seconds)')
plt.ylabel('amplitude  (mV)')
plt.plot(np.array(range(len(d)))/25000, d)
plt.show()

fig, axs = plt.subplots(2)
number_of_activations_in_class=0
count_labels=-1
for i in Index:
 count_labels+=1
 if Class[count_labels]==1:
   number_of_activations_in_class+=1
   axs[0].plot(np.array(range(len(d[i:i+57])))/25000, d[i:i+57])
   plt.show
print('Number of activations in class 1 = ',number_of_activations_in_class)
window=56
hop=1
stft = get_STFT(d[899266:899266+57],window,hop)
spectrogram = np.abs(stft ** 2)
spectrogram = librosa.amplitude_to_db(spectrogram, ref=np.max)
lida.specshow(spectrogram, sr=25000,hop_length=hop, x_axis='s', y_axis='hz');

fig, axs = plt.subplots(2)
number_of_activations_in_class=0
count_labels=-1
for i in Index:
 count_labels+=1
 if Class[count_labels]==2:
   number_of_activations_in_class+=1
   axs[0].plot(np.array(range(len(d[i:i+57])))/25000, d[i:i+57])
   plt.show
print('Number of activations in class 2 = ',number_of_activations_in_class)
window=56
hop=1
stft = get_STFT(d[711074:711074+57],window,hop)
spectrogram = np.abs(stft ** 2)
spectrogram = librosa.amplitude_to_db(spectrogram, ref=np.max)
lida.specshow(spectrogram, sr=25000,hop_length=hop, x_axis='s', y_axis='hz');

fig, axs = plt.subplots(2)
number_of_activations_in_class=0
count_labels=-1
for i in Index:
 count_labels+=1
 if Class[count_labels]==3:
   number_of_activations_in_class+=1
   axs[0].plot(np.array(range(len(d[i:i+57])))/25000, d[i:i+57])
   plt.show
print('Number of activations in class 3 = ',number_of_activations_in_class)
window=56
hop=1
stft = get_STFT(d[753868:753868+57],window,hop)
spectrogram = np.abs(stft ** 2)
spectrogram = librosa.amplitude_to_db(spectrogram, ref=np.max)
lida.specshow(spectrogram, sr=25000,hop_length=hop, x_axis='s', y_axis='hz');

fig, axs = plt.subplots(2)
number_of_activations_in_class=0
count_labels=-1
for i in Index:
 count_labels+=1
 if Class[count_labels]==4:
   number_of_activations_in_class+=1
   axs[0].plot(np.array(range(len(d[i:i+57])))/25000, d[i:i+57])
   plt.show
print('Number of activations in class 4 = ',number_of_activations_in_class)
window=56
hop=1
stft = get_STFT(d[1302019:1302019+57],window,hop)
spectrogram = np.abs(stft ** 2)
spectrogram = librosa.amplitude_to_db(spectrogram, ref=np.max)
lida.specshow(spectrogram, sr=25000,hop_length=hop, x_axis='s', y_axis='hz');

fig, axs = plt.subplots(2)
number_of_activations_in_class=0
count_labels=-1
for i in Index:
 count_labels+=1
 if Class[count_labels]==5:
   number_of_activations_in_class+=1
   axs[0].plot(np.array(range(len(d[i:i+57])))/25000, d[i:i+57])
   plt.show
print('Number of activations in class 5 = ',number_of_activations_in_class)
window=56
hop=1
stft = get_STFT(d[1316598:1316598+57],window,hop)
spectrogram = np.abs(stft ** 2)
spectrogram = librosa.amplitude_to_db(spectrogram, ref=np.max)
lida.specshow(spectrogram, sr=25000,hop_length=hop, x_axis='s', y_axis='hz');

number_of_activations=len(Index)
keys=[]
for j in range(number_of_activations):
  keys.append((Index[j],Class[j]))
print(keys)

#creating additional "no activation" class (where gaps are big), or the noise class number 0
no_activation_Class_indexes=[0]
for j in range(1,number_of_activations):
  if (Index_sorted[j]-Index_sorted[j-1])>700:
    no_activation_Class_indexes.append(Index_sorted[j-1]+100)
print(no_activation_Class_indexes)
print(len(no_activation_Class_indexes))

window=56
hop=1
stft = get_STFT(d[16660:16660+57],window,hop)
spectrogram = np.abs(stft ** 2)
spectrogram = librosa.amplitude_to_db(spectrogram, ref=np.max)
lida.specshow(spectrogram, sr=25000,hop_length=hop, x_axis='s', y_axis='hz');
plt.show

for i in no_activation_Class_indexes:
   plt.pyplot.plot(np.array(d[i:i+100]))
   plt.pyplot.title('Noise')

for j in no_activation_Class_indexes:
  keys.append((j,0))
import random
random.shuffle(keys)
print(keys)

window=37
hop=1
stft = get_STFT(d[1194621:1194621+57],window,hop)
spectrogram = np.abs(stft ** 2)
spectrogram = librosa.amplitude_to_db(spectrogram, ref=np.max)
lida.specshow(spectrogram, sr=25000,hop_length=hop, x_axis='s', y_axis='hz');
plt.title('Spectrogram')
plt.show
type(spectrogram)
spectrogram.shape

#getting spectrogram
def get_spectrogram(data,index,window,hop):
  stft = get_STFT(data[index:index+57],window,hop)
  spectrogram = np.abs(stft ** 2)
  return(librosa.amplitude_to_db(spectrogram, ref=np.max))

#normalizing "pixels"
def spec_to_image(spec, eps=1e-6):
        mean = spec.mean()
        std = spec.std()
        spec_norm = (spec - mean) / (std + eps)
        spec_min, spec_max = spec_norm.min(), spec_norm.max()
        spec_scaled = 255 * (spec_norm - spec_min) / (spec_max - spec_min)
        spec_scaled = spec_scaled.astype(np.uint8)
        return np.array(spec_scaled)

plt.plot(np.array(d[567211:567211+50]))
plt.xlabel('number of samples')
plt.ylabel('amplitude')
plt.title('Amplitudes of a spike')

s=get_spectrogram(d,567211,47,1)
lida.specshow(s, sr=25000, x_axis='s', y_axis='hz');
plt.title('Spectrogram of a spike')
plt.show
s.shape

plt.imshow(spec_to_image(s), interpolation='none')
spec_to_image(s).shape

"""# Prepairing training and validation data"""

window=47
hop=1
train_labels=[]
val_labels=[]
train_data=[]
val_data=[]
count=-1
train_set_limit=len(keys)*6/7
val_set_limit=len(keys)
for key in keys:
  count+=1
  if val_set_limit>count>train_set_limit:
    val_labels.append(key[1])
    spec=spec_to_image(get_spectrogram(d,key[0],window,hop))
    val_data.append(spec)
  if count<train_set_limit:
    train_labels.append(key[1])
    spec=spec_to_image(get_spectrogram(d,key[0],window,hop))
    train_data.append(spec)
num_category = 6
train_data=np.array(train_data)
val_data=np.array(val_data)
# convert class vectors to binary class matrices
print(val_data.shape)
train_data = train_data.reshape(train_data.shape[0], 24, 57, 1)
val_data = val_data.reshape(val_data.shape[0], 24, 57, 1)
print(train_data.shape)
train_labels = keras.utils.to_categorical(train_labels, num_category)
val_labels = keras.utils.to_categorical(val_labels, num_category)
row=24
column=57

"""# Building and compiling the model

Convolution layer with kernel size : 3x3
    Convolution layer with kernel size : 3x3
    Max Pooling layer with pool size : 2x2
    Dropout layer
    Flattening layer
    2 Dense layered Neural Network at the end
"""

number_of_classes=6
input_shape=(row,column,1)
model = Sequential()
#convolutional layer with rectified linear unit activation
model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',input_shape=input_shape))
#32 convolution filters used each of size 3x3
#again
model.add(Conv2D(64, (3, 3), activation='relu'))
#64 convolution filters used each of size 3x3
#choose the best features via pooling
model.add(MaxPool2D(pool_size=(2, 2)))
#randomly turn neurons on and off to improve convergence
model.add(Dropout(0.25))
#flatten since too many dimensions, we only want a classification output
model.add(Flatten())
#fully connected to get all relevant data
model.add(Dense(128, activation='relu'))
#one more dropout for convergence's sake 
model.add(Dropout(0.5))
#output a softmax to squash the matrix into output probabilities
model.add(Dense(number_of_classes, activation='softmax'))

model.compile(loss=keras.losses.categorical_crossentropy,optimizer=Adadelta(),metrics=['accuracy'])

"""# Training"""

#model training
model_log = model.fit(train_data, train_labels,
          batch_size=64,
          epochs=57,
          verbose=1,
          validation_data=(val_data, val_labels))

score = model.evaluate(val_data, val_labels, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

# plotting the metrics
fig = plt.figure()
plt.subplot(2,1,1)
plt.plot(model_log.history['accuracy'])
plt.plot(model_log.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='lower right')
plt.subplot(2,1,2)
plt.plot(model_log.history['loss'])
plt.plot(model_log.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.tight_layout()
fig

#Save the model
# serialize model to JSON
model_digit_json = model.to_json()
with open(data_path+'model_digit_02.json', "w") as json_file:
    json_file.write(model_digit_json)
# serialize weights to HDF5
model.save_weights(data_path+'model_digit_02.h5')
print("Saved model to disk")

#loading the model
json_file = open(data_path+'model_digit.json_01', 'r')
loaded_model_json = json_file.read()
json_file.close()
loaded_model = keras.models.model_from_json(loaded_model_json)
loaded_model.load_weights(data_path+'model_digit_01.h5')
#compiling the model
loaded_model.compile(loss=keras.losses.categorical_crossentropy, optimizer=Adadelta(),metrics=['accuracy'])

"""# Working with new data and predictions"""

mat = spio.loadmat(data_path + 'submission.mat', squeeze_me=True)
submission = mat['d']

plt.title('Signal in real time')
plt.xlabel('time  (seconds)')
plt.ylabel('amplitude  (mV)')
plt.plot(np.array(range(submission[480000:480999].size))/25000, submission[480000:480999])
plt.show()

plt.title('Signal in real time')
plt.xlabel('time  (seconds)')
plt.ylabel('amplitude  (mV)')
plt.plot(np.array(range(submission.size))/25000, submission)
plt.show()

"""Feeding data to the neral network"""

submission_frames=[]
l=submission.size
s=0
while s<l-57:
  submission_frames.append(submission[s:s+57]) #slicing a window for our future spectrogram
  s+=47

submission_frames.append(submission[l-57:])
submission_frames=np.array(submission_frames)
print(submission_frames.shape)

window=47
hop=1
submission_spectr=[]
for h in submission_frames:
    spec=spec_to_image(get_spectrogram(h,0,window,hop))
    submission_spectr.append(spec)
submission_spectr=np.array(submission_spectr)
row=24
column=57

submission_spectr = submission_spectr.reshape(submission_spectr.shape[0], row, column, 1)

results=np.array(model.predict(submission_spectr))

results_=[]
for r in results:
  results_.append(np.argmax(r))
print(results_)

"""Everything is bad with real data"""

predicted_index=[]
predicted_keys=[]
for p in submission_frames:
  predicted_index.append(p[0])
x=0

for w in predicted_index:
  predicted_keys.append((predicted_index,results_[x]))
  x+=1

print(predicted_keys[4])